{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vector database for RAG\n",
    "\n",
    "Build a RAG for Turing College [knowledge base confluence pages](https://turingcollege.atlassian.net/wiki/spaces/DLG/overview) \\\n",
    "so learners can chat some basic questions related to learning in TC with this \\\n",
    "chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv, os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "UPSTASH_TC_HYBRID_CHAT_TOKEN = os.getenv(\"UPSTASH_TC_HYBRID_CHAT_TOKEN\")\n",
    "UPSTASH_TC_HYBRID_INDEX_ENDPOINT = os.getenv(\"UPSTASH_TC_HYBRID_INDEX_ENDPOINT\")\n",
    "\n",
    "UPSTASH_TC_CHAT_DENSE_TOKEN = os.getenv(\"UPSTASH_TC_CHAT_DENSE_TOKEN\")\n",
    "UPSTASH_TC_CHAT_DENSE_ENDPOINT = os.getenv(\"UPSTASH_TC_CHAT_DENSE_ENDPOINT\")\n",
    "\n",
    "UPSTASH_TC_CHAT_DENSE_1024_TOKEN = os.getenv(\"UPSTASH_TC_CHAT_DENSE_1024_TOKEN\")\n",
    "UPSTASH_TC_CHAT_DENSE_1024_ENDPOINT = os.getenv(\"UPSTASH_TC_CHAT_DENSE_1024_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping all links on the [Overview Page](https://turingcollege.atlassian.net/wiki/spaces/DLG/overview)\n",
    "\n",
    "Use BeautifulSoup for scraping, scrape in the context tab:\n",
    "- h1: title of the page\n",
    "- h3: title of a passage\n",
    "- p: a sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = \"https://turingcollege.atlassian.net/wiki/spaces/DLG/overview\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup2 = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "pages = []\n",
    "\n",
    "for tag in soup2.find_all(\"a\"):\n",
    "    title = tag.text\n",
    "    link = tag.get(\"href\")\n",
    "\n",
    "    if link and link.startswith(\"https\") and \"DLG\" in link:\n",
    "        pages.append({\"title\": title, \"link\": link})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape content in the links and chunk the content using RecursiveCharacterTextSplitter\n",
    "\n",
    "__Chunking deliminator__\n",
    "1. Hard Limit: no overlap \n",
    "    - Page\n",
    "    - Passage in page (`h3` tag)\n",
    "\n",
    "2. Soft limit: with overlap\n",
    "    - `. ` End of sentence\n",
    "    - `\\n`\n",
    "    - ' '\n",
    "\n",
    "__Chunk size__\n",
    "- 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "titles_ls = [page[\"title\"] for page in pages]\n",
    "links_ls = [page[\"link\"] for page in pages]\n",
    "\n",
    "articles_chunks_dict_ls = []\n",
    "# loop through the links_ls\n",
    "for article_id in range(len(links_ls)):\n",
    "    response = requests.get(links_ls[article_id])\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    content_div = soup.find(\"div\", class_=\"ak-renderer-document\")\n",
    "    accumulated_article_str = \"\"  # reset the article string\n",
    "\n",
    "    # preserve article subheadings structure\n",
    "    for tag in soup.find_all([\"h1\", \"h3\", \"p\"]):\n",
    "        if tag.name == \"h3\":\n",
    "            tag_name = \"<h3_tag> \"\n",
    "        elif tag.name == \"h1\":\n",
    "            tag_name = \"<h1_tag> \"\n",
    "        else:\n",
    "            tag_name = \"\"\n",
    "        text = tag_name + tag.text + \" \"\n",
    "        accumulated_article_str += text\n",
    "    # add article break tag\n",
    "    accumulated_article_str += \"<article_end_tag> \"\n",
    "\n",
    "    # cleaning up the joined texts\n",
    "    accumulated_article_str = (\n",
    "        accumulated_article_str.replace(\"\\xa0\", \"\")\n",
    "        .replace(\"_______________\\nTuring College\", \"\")\n",
    "        .replace(\"<h3_tag> Analytics \", \"<h3_tag> \")\n",
    "    )\n",
    "\n",
    "    # split the joined text into chunks\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"<article_end_tag>\", \"<h3_tag>\", \". \", \"\\n\", \" \"],\n",
    "    )\n",
    "\n",
    "    splitted_article = splitter.split_text(accumulated_article_str)\n",
    "\n",
    "    # create a dictionary for each chunk\n",
    "    for chunk_id in range(len(splitted_article)):\n",
    "        if len(splitted_article[chunk_id]) < 30:  # exclude empty docs\n",
    "            continue\n",
    "        articles_chunks_dict_ls.append(\n",
    "            {\n",
    "                \"id\": f\"article_{article_id}_chunk_{chunk_id}\",\n",
    "                \"data\": splitted_article[chunk_id],\n",
    "                \"metadata\": {\n",
    "                    \"title\": titles_ls[article_id],\n",
    "                    \"link\": links_ls[article_id],\n",
    "                    \"text\": splitted_article[chunk_id],\n",
    "                },\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsert chunks to Upstash\n",
    "\n",
    "__Embedding model__\n",
    "\n",
    "- SentenceTransformer all-MiniLM-L6-v2\n",
    "- dimension: 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from upstash_vector import Index, Vector\n",
    "\n",
    "# upsert\n",
    "index = Index(url=UPSTASH_TC_HYBRID_INDEX_ENDPOINT, token=UPSTASH_TC_HYBRID_CHAT_TOKEN)\n",
    "\n",
    "for i in articles_chunks_dict_ls:\n",
    "    index.upsert(vectors=[Vector(id=i[\"id\"], data=i[\"data\"], metadata=i[\"metadata\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Retrieval__\n",
    "\n",
    "As we are using internal document, there are some specific organisation internal \\\n",
    "lingo which are OOV to embedding models. Thus we are using a combination of \\\n",
    "dense embedding ANN search and sparse embedding keyword matching search.\n",
    "\n",
    "- Dense embedding cosine similarity search\n",
    "- Sparse embedding BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_ref(query_str, top_k=5):\n",
    "\n",
    "    index = Index(\n",
    "        url=UPSTASH_TC_HYBRID_INDEX_ENDPOINT, token=UPSTASH_TC_HYBRID_CHAT_TOKEN\n",
    "    )\n",
    "\n",
    "    ref_ls = index.query(\n",
    "        data=query_str,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "    )\n",
    "    metadata_ls = [ref.metadata for ref in ref_ls]\n",
    "    return metadata_ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
